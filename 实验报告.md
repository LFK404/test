# 深度学习赛道实验报告
## 1.wsl配置
#### 安装遇到的问题
  &emsp;&emsp;前段时间在王子勤学姐的建议下已经先完成了wsl的下载配置等，虽然知道深度学习不一定要图形化界面，可以只使用命令行，但是我想尝试在wsl的桌面环境下进行开发，于是找了份教程开始安装Ubuntu的WSL GUI App和安装GNOME桌面环境，GUI很顺利，然后在安装桌面环境时出了问题
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-09%20173545.png)
  &emsp;&emsp;安装出了问题，教程里没有，去网上查了后使用`sudo apt-get -f install `来修复，然后再用`sudo apt update`更新，再尝试`sudo apt install xrdp -y`来安装，这次发生了网络解析错误
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-09%20174216.png)
  &emsp;&emsp;然后尝试使用`sudo apt update`，关闭命令行再打开，但尝试了很多次都没用，然后在向hangyi郑福祥学长求助之后，得出是系统在关闭命令行后未彻底关闭，于是直接重启电脑。
  &emsp;&emsp;在成功打开远程桌面后发现并无本应预装的firefox，经搜索后得出是Snap版本异常，使用`sudo snap remove firefox --purge  # 彻底卸载 sudo snap install firefox         # 重新安装`,然后经检验也是成功配置完成。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-09%20200732.png)
  &emsp;&emsp;在稍微使用Ubuntu后我对其不甚满意，主要是其使用系统的远程桌面来操作，导致其使用过程中有较高的延迟，以及不能关闭后台的命令行，要保持原终端开着，嫌占地方。于是决定寻找更适合我的系统，便决定尝试Kali Linux，主要原因是其不需要使用系统的远程桌面，而是使用VCN的远程桌面有不错的优化美化和较低的延迟和帅:smile:，安装过程中依旧报错。然后在登入后按教程设置成中文时，但又遇到了问题。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-10%20014745.png)
  &emsp;&emsp;经查阅后得知是缺少locales包以`sudo apt install locales`安装，之后待到选择界面就又出了问题，一直无法选择，找了好久只查找到按空格来选中，但无法切换，找好久才找到要按Tab键切换到OK
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-10%20015406.png)
  &emsp;&emsp;因嫌每次命令行打开太麻烦便上网搜索，最后选择根据官方文档创建 Windows 终端快捷方式，改写windows terminal的json配置文件，将win-kex给添加到了菜单里，之后就无须在命令行输入kex，可以通过这个下拉菜单打开Kex的图形界面。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-10%20021311.png)
  &emsp;&emsp;之后在进行桌面配置的过程中有教程说要重新登录桌面使输入法配置生效，但一直遇到下面的问题，无法reboot，尝试关闭防火墙、关闭代理、调整网络设置等搜索出的内容都没用，各种检查也全试了个遍
  ，然后不断尝试各种方法，然后疯狂报错，折腾了好久
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-10%20205543.png)
  &emsp;&emsp;最后发现好像是wsl本身无法reboot，教程里的可能是真正的linux系统，我在查找时并未限制要是wsl，于是便放弃了，尝试从别的方面研究是哪出了问题。嗯，错误错误还是错误，连卷和重启都没了，`wsl --unregister Kali-linux`全删了重新下吧，好烦。嗯，配置乱了，再重装系统。这无法访问有些理解了，不同的网总有能访问进去的，开代理一个个试就好。
  &emsp;&emsp;虽然每次试错更改配置总会把配置改得奇奇怪怪的导致到后面为了减少变量，只能重装，重装了好几次，AI也被问得扛不住超时了，不过经过几次重装，也是基本理解了那些命令的作用，知道自己在配置的大概是个啥，后面再装就比较清晰了，那些比较常见的问题也大概知道怎么解决了，然后再回头看一下自己之前写的那些问题，感觉好傻，但就不删了。
  &emsp;&emsp;输入法按了好几个教程的步骤来没有一个可以用的，也是一直查找询问，但基本全是卡在配置无法生效，然后一整天过去了，没有什么进展，昨天以输入法结束今天还是以输入法结束:anger:，去找张润诚学长求助，但可惜在我被叫走前也还是没能成功。第二天早上，找了小恐龙喻鸿杰学长帮忙，但也是在彩彩杨许玮学长的注视下遗憾告退，全都是输入法配置无法生效。在这期间也是有再尝试过调试ubuntu系统，尝试换掉原本的gnome桌面环境，尝试了xfce和kde环境，但高延迟的问题还是没有解决，又或者是刷新率太低，反正用着难受，查阅时发现Ubuntu好像能用VNC服务器，但好像要下载VCN客户端，比较麻烦，最主要是我已经在kali上花那么多的时间了，遇到问题就想办法解决，不能半途而废。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-12%20203024.png)
  &emsp;&emsp;后面郑福祥学长没有采用我们几个之前选择的fcitx框架配fcitx-pingyin输入法或ibus框架配ibus-pingyin输入法，而是先采用了fcitx框架配rime中州韵输入法，第一次在kali中打出中文，但我又发现有些问题，刚启动时不能用，得等几分钟配置才会逐步生效，应该是有不知名的原因导致其输入法配置速度极慢，后面学长尝试更改了输入法，后面用了fcitx自带的汉语输入法，可以快速配置，但等到第二天就又不行了。
  后面我根据一篇知乎上的教程也成功地自己打出了中文，但和之前一样重新启动后就又配置不了，无法切换输入法
  ![]()
  &emsp;&emsp;后面我通过创建~/.xprofile文件，并添加
  ```
  export GTK_IM_MODULE=ibus
  export QT_IM_MODULE=ibus
  export XMODIFIERS=@im=ibus
  #启动 IBus 守护进程
  ibus-daemon -drxIBus
  ```
  &emsp;&emsp;终于成功让输入法配置生效，但一重启就又变回原样了，于是添加自启动的程序，后面候选词窗口没弹出来，大概是因为没装面板组件或被 Wayland 拦了。而我按快捷键看到的应该ibus的引擎列表，实际并没有切换，所以才只能鼠标切换。又在经过各种下载配置置，终于硬控了我3天的输入法终于完美解决了，不用将就了:smile:。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-13%20164107.png)
  &emsp;&emsp;然后为了杀死这个弹窗，保留 Wayland 会话，把通知守护进程换成支持 wlr-layer-shell的mako，然后一通乱改，结果最后告诉我:你的 Wayland 合成器缺协议，修不了。算了反正改后那个窗口也是不弹了，把多余的东西删了就走吧。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-13%20163025.png)
  &emsp;&emsp;第二天我去研究了一下kali的官方英文文档，然后就注意到了里面有一个无锋模式，可以在windows桌面的背景下使用kali内的软件(感觉描述得不是很准确)，还有终端快捷方式。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-19%20190541.png)
  &emsp;&emsp;然后按官方英文教程后第一次是成功通过快捷方式打开了无缝模式，但之后即使是命令行输入也无法打开，只能开启最基本的窗口模式，经过研究后好像是我之前的设置还是怎样，导致我的wsl每次开启都会换一个ip，导致快捷方式只能用一次，和ai搞了半天也没搞定，最后又找上了福祥学长，福祥学长搞了一会后觉得是我之前配得有问题，于是直接给我重装了(可怜我搞了好久的输入法:cry:),重装后经过几个小时的奋战，中途也是有成功开启过几次无缝窗口，但快捷方式一直用不了，等后面命令行也没法开无缝模式了，最后只能窗口模式将就用了:cry:。
#### 总结
  **(前面话好像有点多了，不知道学长看不看得下去，就来了个总结)**
  &emsp;&emsp;虽然知道深度学习不一定要图形化界面，可以只使用命令行，但是我想尝试在wsl的桌面环境下进行开发，于是便研究起了wsl的图形化桌面环境，后因不满Ubuntu默认桌面环境的风格、布局、高延迟，选择了去折腾kali，输入法的配置以及无缝桌面模式是途中遇到的两个最大的问题，叫学长帮忙也没解决，最后也只是靠自己解决了输入法的配置，无缝模式最终也是只能放弃了。这几天在折腾kali和Ubuntu的过程中也是遇到了一堆问题，因为爱瞎搞乱配，又想排除前面操作的变量，两个系统加起来重装了十几次，对wsl的熟悉度也是大大提高，很多没见过的命令也都大概知道是干啥的了，常见问题应该也是全踩了，是学到了很多，不过花的时间也很多，报的东西有些多最近忙的很，后面的实验也是得抓紧了，但愿来得急。
## 2.数据库选择
  &emsp;&emsp;接下来打算先选择数据库，根据题目要求模型最后输出的好/中/差评作为原始模型的分类标准，那么最理想的情况下应该是挑一三分为好评中评差评的中文商品评论三分类数据集，但我搜索到的中文商品评论数据集基本全是只分正负的二分类数据集，偶尔找到的那么两三个表面写着三分类的数据集，点进去要么是文件已失效，要么其实是二分类的，要么是文件不是csv格式的。又问了几个ai，但不知道他们咋回事，无论是kimi、deepseek还是gpt-5、copilot，全都集体抽风了，给出来的链接全都不能用，打开全是全是404。最后在跟ai扯皮的时候注意到了ASAP数据集。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-19%20201103.png)
  &emsp;&emsp;这是一个把评论分为1-5星级的数据集，还有1（积极）、0（中性）、-1（负面）、-2（未提及）的标签，虽然不是我一开始打算的三分类数据集，但也不是不能用，只要只要以情绪字段为优先，未提及的情况下再把5-4分为好评，3为中评，2-1为差评就行了，然后ai给的链接指向一个别的网站，但也是不能用，不过最后我发现了ASAP数据集在GitHub上也有，而且就连训练集和测试集他也给我分好了，不用我再分了。就是不知道为啥复制ASAP数据集里的train.csv的直链时，它一直给我直接下载文件，不过好在另一个test.csv的直链可以复制，我就自己给它把末尾手动改成train.csv吧。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-20%20125911.png)
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-19%20191253.png)
  
## 3.程序模型下载
  &emsp;&emsp;接下来是要准备下载实验相关的东西了，不过下载之前，得先重装一下，福祥老师之前把这系统托盘和桌面弄脱节了，得弄回去，不过现在重装比以前熟练多了，之前重装动不动就一两个小时，杂七杂八的弄一堆，不配置输入法的话现在十来分钟就能搞定基本的桌面设置了。接下来是运行python的编辑器，我选择用vscode。本来是打算使用wsl的vscode集成用wsl扩展在Windows主系统上进行远程运行，但想了想我都安装图形化桌面了还是直接在kali里下一个更方便快捷。

  &emsp;&emsp;**然后从这开始本实验报告都将在kali里的vscode中进行编写。**

  &emsp;&emsp;然后就又遇到了一个问题，我的GitHub的登录状态无法保存，每次重启vscode都需要重新登录 
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-20%20161941.png)
  &emsp;&emsp;查询后先是把`gnome-keyring-daemon --start --components=secrets`写入了 ~/.xprofile  里，确保每次登录都启动。然后失败了，连输入法都出了问题，赶紧删了换成安装  libsecret  并配置一个兼容的 keyring,`sudo apt install libsecret-1-0 libsecret-1-dev build-essential git`然后跟ai搞了好久，最后发现是钥匙环被锁了，于是一个`printf '123456\n' | gnome-keyring-daemon --unlock --components=secrets`然后又是D-Bus 的  org.freedesktop.secrets  服务没有启动成功，也就是 gnome-keyring-daemon 没跑起来，一通瞎搞后，嗯，完蛋了，没救了，重开，不过倒是越装越快了:cry:。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-13%20173408.png)
  &emsp;&emsp;前面花的时间太多了，没时间继续研究到底是怎样了，只能先将就用着了，反正copilot也能用，就是每次用都得重新登录，难受。
  &emsp;&emsp;接下来是CUDA和PyTorch的安装。一开始，CUDA和PyTorch的版本我都按教程没有选最新的，说是兼容性强，BUG少，然后CUDA用了12.4版本，PyTorch用了2.6.0，嗯，然后不出所料的出错了。
  ![]()
  &emsp;&emsp;PyTorch 2.6.0太老了，只支持到40系的sm_90，而我的5070是sm_120，查阅后我决定直接把CUDA和PyTorch都删了，直接都升级到最新版本，反正官方网站都写了能兼容，什么低版本兼容性强见鬼去吧，出了问题再解决就好了，不可能只有我有问题。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20003606.png)
  &emsp;&emsp;都是在官方网站弄的代码，整个python与CUDA不兼容的问题处理起来跟配置输入法、无缝模式、gnome-keyring兼容这三个问题比起来简直不要太轻松:smile:，他喵那三个是什么鬼玩意:anger:。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20004459.png)
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20005357.png)
  &emsp;&emsp;第一次python虚拟环境是在命令行上创的，这次改成用vscode来创建，虽然我目前就一个项目没谁能干扰，但不在虚拟环境里没法用pip安装。CUDA装到13.0，PyTorch装到2.9.0。
  ![]()
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20020939.png)
  &emsp;&emsp;然后第二天，我的桌面莫名奇妙打不开了:cry:，在那边卡半天然后就是一个错误
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-20%20203457.png)
  &emsp;&emsp;我又尝试了其他模式无缝模式倒是能打开，但是开了几秒就会闪退，增强会话模式也还是黑屏，然后开始跟ai扯皮，扯了半天终于知道是NVIDIA 驱动在WSL2中与TigerVNC冲突了，
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20130219.png)
  &emsp;&emsp;最后在小恐龙学长的劝诫下，重装吧:smile:。
  &emsp;&emsp;重装后，我又盯上了VScode的GitHub登录问题，先是去查阅了VScode的官方英文文档，然后是找到了下面这一段
  ```
  GNOME or UNITY (or similar)

  If the error you're seeing is "Cannot create an item in a locked collection", chances are your keyring's Login keyring is locked. You should launch your OS's keyring (Seahorse is the commonly used GUI for seeing keyrings) and ensure the default keyring (usually referred to as Login keyring) is unlocked. This keyring needs to be unlocked when you log into your system.
  ```
  但其给的Seahorse网站被停用了。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20161616.png)
  &emsp;&emsp;后面是用命令行安装的`sudo apt install seahorse`，
  经过一番折腾，把`gnome-keyring`和`libsecret`也都装上了。
  ![](https://raw.githubusercontent.com/LFK404/test/refs/heads/main/%E5%B1%8F%E5%B9%95%E6%88%AA%E5%9B%BE%202025-10-21%20163046.png)
  &emsp;&emsp;然后又把清除配置了很多环境变量，最后忙活了半天终于是成功的解决了VScode的GitHub登录问题。甚至本来我的vscode有个小毛病，打开后会有半分钟的时间会卡着无法操作，这下也跟着解决了，打开就能直接操作了，意外之喜:smile:。
  &emsp;&emsp;接下来是下载实验所需的模型和代码，先安装一下要用到的几个库`pip install transformers torch pandas scikit-learn`，再用os.environ['HF_ENDPOINT'] = 'https://hf-mirror.com'切换到镜像网页，虽然我接的是magic切不切镜像好像没啥影响。下载Hugging Face上的bert-base-chinese模型BertForSequenceClassification还有分词器BertTokenizer，统一下到文件夹chinese_bert/里，就是我`pip3 install torch torchvision --index-url https://download.pytorch.org/whl/cu130`下的cu130怎么变成cu128了
  ```
  ┌──(.venv)(lfk㉿LAPTOP-HUDV3PUK)-[~/Desktop/2025-Autumn-Assessment-Questions]
  └─$ python -c "import torch; print(torch.__version__)"                            
  2.9.0+cu128

  ┌──(.venv)(lfk㉿LAPTOP-HUDV3PUK)-[~/Desktop/2025-Autumn-Assessment-Questions]
  └─$ python -c "import torch; print('CUDA available:', torch.cuda.is_available())" 
  CUDA available: True
  ```
  &emsp;&emsp;应该没啥影响吧，算了算了，还是卸了重下吧。
## 4.实验代码编写
  &emsp;&emsp;接下来是编写实验代码了，看一下应该主要是分为数据处理、模型训练、训练评估三个部分，当然肯定也是报错不断，一会这里少了个库，一会哪里少写了个参数，但这些报错都还好，只要缺啥补啥、错啥改啥就都可以搞定，没像上面的那么变态，最后也总算是把程序跑通了。
  ![]()
  &emsp;&emsp;数据处理主要是读取ASAP数据集的train.csv和test.csv文件，然后使用BertTokenizer对评论文本进行分词和编码，最后将数据转换为PyTorch的Dataset格式，方便后续的训练和评估。
  &emsp;&emsp;首先是数据清洗，既要移除多余的东西，但又不能清理得太狠，避免改得面目全非。于是便用正则表达式来移除除中文、标点、字母、数字之外的东西。
  ```python
  def clean_text(text):
    if not isinstance(text, str):
        return ""
    text = re.sub(r'[^\w\u4e00-\u9fff，。！？；：""\']', '', text)
    text = re.sub(r'\s+', ' ', text)
    return text.strip()
  ```
  &emsp;&emsp;然后是数据分类，原始数据是5星评分，但我们需要3分类，所以我决定先把3星归为中评，5-4星归为好评，2-1星归为差评，等运行后看结果如何，需要的话再调整。
  ```python
  def make_three_class(df):
    star_to_label = {5: 2, 4: 2, 3: 1, 2: 0, 1: 0}
    df['rating'] = df['star'].map(star_to_label)
    df = df.dropna(subset=['rating'])
    return df
  ```
  &emsp;&emsp;接下来是数据集的划分，虽然有了测试集做最后的评估，但还是得从训练集中划分出一部分作为验证集，用于早停，避免过拟合,还得分层抽样，保证各类别比例一致，还得设置随机种子保证可复现。
  ```python
    train_df, val_df = train_test_split(
        train_df[['review_clean', 'rating']],
        test_size=0.2,
        random_state=42,
        stratify=train_df['rating'] )
  ```
  &emsp;&emsp;然后是得处理数据类别的不平衡，有两个想法，一个是强制匹配，固定一个好评一个差评一个中评，但这样会丢弃很多数据，另一个是使用类别权重，在计算损失时给予少数类更高的权重，避免模型偏向多数类，我选择了后者。
  ```python
  class_weights = compute_class_weight(
    'balanced',
    classes=np.array([0, 1, 2]),
    y=train_df['rating'].values
)
  ```
  &emsp;&emsp;模型训练部分主要是加载预训练的BertForSequenceClassification模型，并根据需要调整分类头的输出维度，以适应三分类任务。
  &emsp;&emsp;首先是加载各种东西
  ```python
    # 加载模型和分词器
  tokenizer = BertTokenizer.from_pretrained('./chinese_bert')
  model = BertForSequenceClassification.from_pretrained('./chinese_bert', num_labels=3)
  model.to(device)

  # 加载训练数据和验证数据
  train_df = pd.read_csv('train_data.csv')
  val_df = pd.read_csv('val_data.csv')
  print(f"训练集: {len(train_df)}条, 验证集: {len(val_df)}条")

  # 加载类别权重
  class_weights = np.load('class_weights.npy')
  print("类别权重:", class_weights)
  ```
  &emsp;&emsp;接下来使用BERT分词器对评论文本进行编码，将文本和标签编码为BERT模型需要的格式，把文本转换成数字，返回PyTorch张量格式。还有就是太长的文本需要截断，不够长的需要补齐，太短可能丢失重要信息，太长计算效率低，而且BERT对输入长度有限制，最长512个token，而ASAP数据集里基本全是超长段，一句顶别人几句，必须截断，这里我设置为128，应该够用了吧。
  ```python
  def encode_texts(texts, labels):
    encodings = tokenizer(
        texts, 
        truncation=True, 
        padding=True, 
        max_length=128, 
        return_tensors='pt'
    )
    return {
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'], 
        'labels': torch.tensor(labels, dtype=torch.long)
    }
  ```
  &emsp;&emsp;然后是创建自定义的Dataset类，注意力掩码还是很重要的，能告诉模型哪些位置是真实的文本内容，哪些是填充的。这样模型就会关注真实文本部分，忽略填充部分。为此我定义了一个数据集类，支持按索引批量读取数据，利用GPU的并行计算能力，让数据加载更高效。
  ```python
    # 创建数据集
  class ReviewDataset(torch.utils.data.Dataset):
      def __init__(self, encodings):
          self.encodings = encodings
      def __len__(self):
          return len(self.encodings['labels'])
      def __getitem__(self, idx):
          return {k: v[idx] for k, v in self.encodings.items()}

  train_dataset = ReviewDataset(train_encodings)
  val_dataset = ReviewDataset(val_encodings)
  ```
  &emsp;&emsp;数据存在明显的类别不平衡，好评数量远多于差评。如果直接训练，模型会"偷懒"，只学会预测多数类，导致少数类的准确率极低。我采用加权损失函数来解决这个问题，给样本少的类别更高权重。
  ```python
  # 自定义Trainer以处理类别不平衡 
class WeightedTrainer(Trainer):
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
        # 应用类别权重
        loss_fct = torch.nn.CrossEntropyLoss(
            weight=torch.tensor(class_weights, dtype=torch.float).to(device)
        )
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss
  ```
  &emsp;&emsp;然后是定义训练参数，知道每个参数是干啥的，但毕竟没试过就先按着教程的量来吧。
  ```python
    training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True,
    weight_decay=0.01,
    warmup_steps=100,
    logging_steps=50,)
  ```
  &emsp;&emsp;训练ai模型有个常见问题：过拟合，就是模型在训练集上表现很好，但在新数据上表现很差。为此我设置了早停回调，当验证集性能连续3轮没有提升时就停止训练。
  ```python
    callbacks=[EarlyStoppingCallback(
    early_stopping_patience=3,
    early_stopping_threshold=0.001
)]
  ```
  &emsp;&emsp;训练评估部分主要是使用训练好的模型在测试集上进行评估，计算准确率、精确率、召回率和F1分数等指标，全面衡量模型性能。
  &emsp;&emsp;模型加载部分的部分本来是这样的
  ```python
    tokenizer = AutoTokenizer.from_pretrained('./chinese_bert')
    model = AutoModelForSequenceClassification.from_pretrained('./chinese_bert')
    model.to(device)
  ```
  &emsp;&emsp;但后面改为了用pipeline来简化模型的加载和预测，封装了模型加载、预处理和后处理，大大简化了代码。
  ```python
    base_clf = pipeline('text-classification', 
                   model='./chinese_bert', 
                   tokenizer='./chinese_bert')
  ```
  
  ```python
    base_preds = []
for text in test_df['review_clean']:
    result = base_clf(text) 
    label = result[0]['label'] 
    base_preds.append(int(label))
  ```
  &emsp;&emsp;也是改成了
  ```python
    base_preds = [int(base_clf(text)[0]['label']) for text in test_df['review_clean']]
  ```
  &emsp;&emsp;最后是计算各种评估指标，准确率、精确率、召回率和F1分数，全面衡量模型性能，这里我就用了sklearn的classification_report函数，一次性输出所有指标，生成详细的分类报告。
  ```python
  classification_report(test_df['rating'], base_preds, 
                     target_names=['差评', '中评', '好评'], digits=4)
  ```
  &emsp;&emsp;糟糕，考核说要是自己的仓库，我之前弄的却是之前克隆的超算的仓库，得改成自己的仓库才行，赶紧新建个仓库把代码转过去吧。
  &emsp;&emsp;然后是正式运行代码，在等了快一个小时后终于是跑完了，结果，嗯，很糟糕
  ```
  基础模型准确率: 0.0684
微调模型准确率: 0.7864
准确率提升: 0.7180

基础模型详细报告:
              precision    recall  f1-score   support

          差评     0.0684    1.0000    0.1281       338
          中评     0.0000    0.0000    0.0000       717
          好评     0.0000    0.0000    0.0000      3885

    accuracy                         0.0684      4940
   macro avg     0.0228    0.3333    0.0427      4940
weighted avg     0.0047    0.0684    0.0088      4940

微调模型详细报告:
              precision    recall  f1-score   support

          差评     0.0000    0.0000    0.0000       338
          中评     0.0000    0.0000    0.0000       717
          好评     0.7864    1.0000    0.8805      3885

    accuracy                         0.7864      4940
   macro avg     0.2621    0.3333    0.2935      4940
weighted avg     0.6185    0.7864    0.6924      4940

  ```
  &emsp;&emsp;这个样子百分百是类别权重没起作用，模型完全偏向了好评类别，差评和中评全都预测成了好评，可能是标签映射错误，也可能是权重没输进去。
  &emsp;&emsp;先让其打印几个预测结果，果然都错了，在trained_model/目录下，打开config.json，映射没问题呀再打开results/目录下的trainer_state.json，可以看出loss值有变说明模型是有训练成功的，加载的模型也没问题，然后让他打印一下类别权重是没问题的。那么是star_to_label这些标签映射的代码的问题，不知道是格式错了还是咋的，导致标签映射失败，就把这些标签映射相关的改一下吧。
  ```python
  def star_to_label(star):
        star = int(star)  # 确保是整数
        if star in [1, 2]:
            return 0  # 差评
        elif star == 3:
            return 1  # 中评
        elif star in [4, 5]:
            return 2  # 好评
        else:
            return 1  # 默认中评
  ```
  &emsp;&emsp;然后重新运行，em...对了一些，但不多。
  ```python
商品评价情感分析模型评估结果
==================================================
测试集大小: 4940
基线模型准确率: 0.4913
微调模型准确率: 0.7864
准确率提升: 0.2951

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

微调模型分类报告:
              precision    recall  f1-score   support

          差评     0.0000    0.0000    0.0000       338
          中评     0.0000    0.0000    0.0000       717
          好评     0.7864    1.0000    0.8805      3885

    accuracy                         0.7864      4940
   macro avg     0.2621    0.3333    0.2935      4940
weighted avg     0.6185    0.7864    0.6924      4940
```
  &emsp;&emsp;我现在的结果保存每次都会覆盖前一次的结果，我想要可以让每一次的评估结果都可以保存到model-result里，而不是每次写入都会覆盖，这样方便进行对比和分析。先要把写入方式，从'w'改为'a'使用追加模式，然后让他每次写入前读取一下文件中已有的第几次评估的行数，然后在写入时加1，这样就能给每一次的结果前加上序号了。
  ```python
  if os.path.exists('model_results.txt'):
    with open('model_results.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
        # 计算文件中已有的"第X次评估"行数
        count = sum(1 for line in lines if line.startswith('第') and '次评估' in line)
        eval_number = count + 1
else:
    eval_number = 1
  ```
  &emsp;&emsp;微调模型差评和中评还是全错，出现了类别不平衡导致的过拟合，但基线模型看样子应该是没问题的，这样看来这次的数据处理是没问题的，问题是出在和微调模型有关的代码上，，是类别权重不够大，还是训练过程中没有有效地处理类别不平衡？
  &emsp;&emsp;首先类别权重与标签编码是对应的，等等会不会是我写的`metric_for_best_model='accuracy'`导致的，因为我的好评实在太不平衡，无论咋训都不如直接全蒙好评来得准确率高，也是，准确率这个指标在类别不平衡时不可靠，模型可能只学会预测多数类，从而获得高准确率，改成`f1_macro`试试吧。哦报错了，compute_metrics也得改。嗯，这结果像样了点，就是怎么没有中评。
  ``` python
第2次评估
测试集大小: 4940
基线模型准确率: 0.4913
微调模型准确率: 0.7905
准确率提升: 0.2992

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

微调模型分类报告:
              precision    recall  f1-score   support

          差评     0.3856    0.4438    0.4127       338
          中评     0.0000    0.0000    0.0000       717
          好评     0.8258    0.9665    0.8907      3885

    accuracy                         0.7905      4940
   macro avg     0.4038    0.4701    0.4344      4940
weighted avg     0.6758    0.7905    0.7287      4940
  ```
  &emsp;&emsp;，而且连续两次都是只训练4次就停了，应该是因为早停回调，看来`early_stopping_threshold=0.001`还是有点高，改成`early_stopping_threshold=0.0001`看下。
  &emsp;&emsp;不过得先改改这训练参数，教程还是太保守了，显存才用了3g，只要显卡不冒烟就往死里跑，换个激进点的方案。训练批次大小翻4倍，学习率策略改为`cosine`控制学习率，后期更好收敛，学习率调度策略Warmup改为按比例，引入的优化版AdamW优化器`adamw_torch_fused`将多个计算操作融合成单个GPU内核调用，`save_total_limit=2`只保留2个最佳模型，节省空间，再来一个weight_decay权重衰减，防止又像上面一样过拟合，先设0.001试一下，再来一个tensorBoard可视化，实时监控训练过程
  ``` python
  training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=15,
    per_device_train_batch_size=64,
    per_device_eval_batch_size=128,
    lr_scheduler_type='cosine',
    warmup_ratio=0.1, 
    weight_decay=0.01, 
    fp16=True,
    gradient_accumulation_steps=2,
    eval_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    max_grad_norm=1.0,
    optim='adamw_torch_fused',
    logging_dir='./logs', 
    logging_steps=50,
    report_to=['tensorboard'], 
)
  ```
  &emsp;&emsp;然后这次训练，由于批次增大，迭代速度从7it多每秒降到了2it多，但在多了5epoch的情况下，迭代总数从18000多降到了3000多，速度快了不少，不过显存只从2g多加到5g多，还能再提一点。虽然这次的结果中评是有了，但是准确率下降了。
  ```python
  第3次评估
测试集大小: 4940
基线模型准确率: 0.4913
微调模型准确率: 0.7482
准确率提升: 0.2569

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

微调模型分类报告:
              precision    recall  f1-score   support

          差评     0.5940    0.6450    0.6184       338
          中评     0.3031    0.4561    0.3641       717
          好评     0.9018    0.8111    0.8540      3885

    accuracy                         0.7482      4940
   macro avg     0.5996    0.6374    0.6122      4940
weighted avg     0.7939    0.7482    0.7668      4940

--------------------------------------------------
  ```
  &emsp;&emsp;然后批次再大点，再移除梯度累积，减少等待时间，移除tensorboard可视化相关的，不好看，还有早停在我把耐心值提到5轮的情况下只跑到第7轮就停了，看来`early_stopping_threshold=0.0001`还是太大了，删掉吧，然后耐心值改成3轮。
  ``` python
  training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=15,
    per_device_train_batch_size=96,  
    per_device_eval_batch_size=128,
    lr_scheduler_type='cosine',
    warmup_ratio=0.1, 
    weight_decay=0.01, 
    fp16=True,
    eval_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=2,
    load_best_model_at_end=True,
    metric_for_best_model='f1',
    max_grad_norm=1.0,
    optim='adamw_torch_fused',
    dataloader_num_workers=6,  
)
  ```
  ``` python
  --------------------------------------------------

第4次评估
测试集大小: 4940
基线模型准确率: 0.4913
微调模型准确率: 0.8061
准确率提升: 0.3148

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

微调模型分类报告:
              precision    recall  f1-score   support

          差评     0.6621    0.5740    0.6149       338
          中评     0.3776    0.3013    0.3351       717
          好评     0.8766    0.9194    0.8975      3885

    accuracy                         0.8061      4940
   macro avg     0.6388    0.5982    0.6158      4940
weighted avg     0.7895    0.8061    0.7965      4940

--------------------------------------------------
```
  &emsp;&emsp;又高了点。不过中评的表现还是不太理想，精确率和召回率都比较低，先试一下把宏平均改成加权平均看看怎样。
  ``` python
  --------------------------------------------------

第5次评估
测试集大小: 4940
基线模型准确率: 0.4913
微调模型准确率: 0.7854
准确率提升: 0.2941

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

微调模型分类报告:
              precision    recall  f1-score   support

          差评     0.6655    0.5473    0.6006       338
          中评     0.3381    0.3612    0.3493       717
          好评     0.8819    0.8844    0.8832      3885

    accuracy                         0.7854      4940
   macro avg     0.6285    0.5977    0.6110      4940
weighted avg     0.7882    0.7854    0.7864      4940

--------------------------------------------------
  ```
  &emsp;&emsp;确定没有其它变量，所以加权平均不如宏平均，就是只训了5轮就触发早停了，算了改成4轮吧。
  &emsp;&emsp;然后训练参数经过多次改动，从一开始的要快小时才能训完，到现在十几分钟就能训完，可惜时间不允许，这个速度勉强够用了，不然我还想试试设段代码来自动对比不同参数下的模型训练速度。
  ``` python
  --------------------------------------------------

第6次评估
测试集大小: 4940
基线模型准确率: 0.4913
微调模型准确率: 0.7370
准确率提升: 0.2457

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

微调模型分类报告:
              precision    recall  f1-score   support

          差评     0.6254    0.5533    0.5871       338
          中评     0.2949    0.4951    0.3696       717
          好评     0.9017    0.7977    0.8465      3885

    accuracy                         0.7370      4940
   macro avg     0.6073    0.6154    0.6011      4940
weighted avg     0.7947    0.7370    0.7595      4940

--------------------------------------------------
  ```
  &emsp;&emsp;结果我本是以防万一出了什么问题，就换回宏平均重新训了一下，因为我觉得在数据如此不平衡的情况下，宏平均的结果应该不如加权平均好，就再确认一下。然后真低了，我在第5与第6次评估间什么都没改，只改了评估指标，而第4次与第5次评估间我除了把宏平均改成了加权平均外应该只改了`early_stopping_patience`与`num_train_epochs`才对，不应该会有这么大的影响呀，然后这一来一回太麻烦了，我决定把代码改成能同时输出两种评估指标的结果。因为我是要保存两个模型所以`load_best_model_at_end`得删去，然后以
  ```python
  def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        if metrics and 'eval_f1_macro' in metrics:
            current_f1_macro = metrics['eval_f1_macro']
            if current_f1_macro > self.best_f1_macro:
                self.best_f1_macro = current_f1_macro
                print(f"新的宏平均F1最佳模型: {current_f1_macro:.4f}")
                output_dir = os.path.join(args.output_dir, "best_f1_macro")
                self.save_model(output_dir)
                tokenizer.save_pretrained(output_dir)
        
        if metrics and 'eval_f1_weighted' in metrics:
            current_f1_weighted = metrics['eval_f1_weighted']
            if current_f1_weighted > self.best_f1_weighted:
                self.best_f1_weighted = current_f1_weighted
                print(f"新的加权平均F1最佳模型: {current_f1_weighted:.4f}")
                output_dir = os.path.join(args.output_dir, "best_f1_weighted")
                self.save_model(output_dir)
                tokenizer.save_pretrained(output_dir)
  ```
  &emsp;&emsp;来手动保存，`metric_for_best_model`倒是得留下，再以宏平均为指标，不然无法早停。
  训练评估部分也得改一下，大概就是读取到我自己保存两个模型的文件夹，然后像之前一样输出写入就行了。
  ``` python
  ```
  &emsp;&emsp;接下来改一下这数据类别，不然这好评实在太多了，改成5星为好评，4-3星为中评，1-2星为差评，但这数据集里的差评也太少了吧，算了，还是5星为好评，4星为中评，1-3星为差评吧。
  &emsp;&emsp;结果等要跑的时候vscode突然识别不出我的虚拟环境了，搞了半天才终于是又加载上，结果它又开始报错，本来能运行的程序突然说找不到chinese_bert，真是服了，这是我的问题还是系统的问题。搞了半天又是成功打开了，虽然只剩下我之前提交的代码了，然后不知道是不是这wsl的网不太行，我之前提交在本地的代码一直无法上传到github，搞了半天也没交上去，然后发现几十个g怎么传。然后想先去继续搞代码，结果网他喵是直接没了，要被气死了:anger:。搞了半天也终于是搞好了，才怪，代码跑不了一点。
  &emsp;&emsp;快不行了，要被这报错搞崩了，不是缺啥就是找不到啥:anger:
  &emsp;&emsp;几个小时了，它终于跑起来了:smile:，然后他喵的没啥时间了:cry:。
  ``` python
  --------------------------------------------------

第7次评估
测试集大小: 4940
=== 基线模型 ===
准确率: 0.3990

=== 宏平均最佳模型 ===
准确率: 0.5814
相比基线 - 准确率提升: 0.1824

=== 加权平均最佳模型 ===
准确率: 0.5832
相比基线 - 准确率提升: 0.1842

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.1873    0.0531    0.0827      1055
          中评     0.3965    0.4215    0.4086      1867
          好评     0.4247    0.5590    0.4827      2018

    accuracy                         0.3990      4940
   macro avg     0.3362    0.3445    0.3247      4940
weighted avg     0.3633    0.3990    0.3693      4940

宏平均最佳模型分类报告:
              precision    recall  f1-score   support

          差评     0.5562    0.6474    0.5983      1055
          中评     0.5203    0.4467    0.4807      1867
          好评     0.6425    0.6715    0.6567      2018

    accuracy                         0.5814      4940
   macro avg     0.5730    0.5885    0.5786      4940
weighted avg     0.5779    0.5814    0.5777      4940

加权平均最佳模型分类报告:
              precision    recall  f1-score   support

          差评     0.6874    0.4607    0.5516      1055
          中评     0.5000    0.5811    0.5375      1867
          好评     0.6350    0.6492    0.6420      2018

    accuracy                         0.5832      4940
   macro avg     0.6075    0.5637    0.5771      4940
weighted avg     0.5952    0.5832    0.5832      4940

==================================================
  ```
  &emsp;&emsp;然后问题有点大呢，两个模型的准确率都差不多，都是58%左右，因为测试集类别分布相对均衡，导致加权平均与宏平均差异不大。都比之前低了将近20个百分点，看来是数据类别划分的问题，是这么分之后不太分明吗。
  ``` python
  成功加载数据：训练集36850条，测试集4940条
数据处理完成！
训练集标签分布： {0: 6174, 1: 10690, 2: 12616}
验证集标签分布： {0: 1544, 1: 2672, 2: 3154}
测试集标签分布： {0: 1055, 1: 1867, 2: 2018}
类别权重: [1.59162078 0.91923916 0.77890509]
```
``` python
使用设备: cuda
训练集: 29480条, 验证集: 7370条
训练集分布: {0: 6174, 1: 10690, 2: 12616}
验证集分布: {0: 1544, 1: 2672, 2: 3154}
原始类别权重: [1.59162078 0.91923916 0.77890509]
加强后类别权重: [4.77486233 2.75771749 2.33671528]
```
类别上是没像之前那么不平衡了，但是模型的准确率却大降，再试试5星好评，4-3星中评，1-2星差评吧。
  ``` python
  ==================================================

第9次评估
测试集大小: 4940
=== 基线模型 ===
准确率: 0.4508

=== 宏平均最佳模型 ===
准确率: 0.6490
相比基线 - 准确率提升: 0.1982

=== 加权平均最佳模型 ===
准确率: 0.6468
相比基线 - 准确率提升: 0.1960

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.5471    0.4203    0.4754      2584
          好评     0.4247    0.5590    0.4827      2018

    accuracy                         0.4508      4940
   macro avg     0.3384    0.3392    0.3330      4940
weighted avg     0.4626    0.4508    0.4486      4940

宏平均最佳模型分类报告:
              precision    recall  f1-score   support

          差评     0.5738    0.6213    0.5966       338
          中评     0.7069    0.5797    0.6370      2584
          好评     0.6102    0.7423    0.6698      2018

    accuracy                         0.6490      4940
   macro avg     0.6303    0.6478    0.6345      4940
weighted avg     0.6583    0.6490    0.6477      4940

加权平均最佳模型分类报告:
              precision    recall  f1-score   support

          差评     0.6183    0.5799    0.5985       338
          中评     0.6712    0.6556    0.6633      2584
          好评     0.6217    0.6467    0.6340      2018

    accuracy                         0.6468      4940
   macro avg     0.6371    0.6274    0.6319      4940
weighted avg     0.6473    0.6468    0.6469      4940

==================================================
  ```
  要好不少，但还是不如4-5星好评，3星中评，1-2星差评的划分方式，那么就以这个为准了，数据类别划分对模型性能的影响还是挺大的。
  ```python
  ==================================================

第10次评估
测试集大小: 4940
=== 基线模型 ===
准确率: 0.4913

=== 宏平均最佳模型 ===
准确率: 0.7370
相比基线 - 准确率提升: 0.2457

=== 加权平均最佳模型 ===
准确率: 0.7370
相比基线 - 准确率提升: 0.2457

基线模型分类报告:
              precision    recall  f1-score   support

          差评     0.0435    0.0385    0.0408       338
          中评     0.1506    0.4170    0.2213       717
          好评     0.7963    0.5444    0.6467      3885

    accuracy                         0.4913      4940
   macro avg     0.3301    0.3333    0.3029      4940
weighted avg     0.6511    0.4913    0.5435      4940

宏平均最佳模型分类报告:
              precision    recall  f1-score   support

          差评     0.6254    0.5533    0.5871       338
          中评     0.2949    0.4951    0.3696       717
          好评     0.9017    0.7977    0.8465      3885

    accuracy                         0.7370      4940
   macro avg     0.6073    0.6154    0.6011      4940
weighted avg     0.7947    0.7370    0.7595      4940

加权平均最佳模型分类报告:
              precision    recall  f1-score   support

          差评     0.6254    0.5533    0.5871       338
          中评     0.2949    0.4951    0.3696       717
          好评     0.9017    0.7977    0.8465      3885

    accuracy                         0.7370      4940
   macro avg     0.6073    0.6154    0.6011      4940
weighted avg     0.7947    0.7370    0.7595      4940

==================================================
  ```
  &emsp;&emsp;那么就以这个1-2、3、4-5为准了。
  &emsp;&emsp;已确定数据那么接下来就进行数据增强了，
  然后没时间了，先交吧，剩下的到github上看自述文件吧，最近实在太忙了，已经尽力做了:cry:。