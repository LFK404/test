<<<<<<< HEAD
=======
# train_model.py
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
import torch
import pandas as pd
import numpy as np
from transformers import (
    BertTokenizer, BertForSequenceClassification, 
<<<<<<< HEAD
    Trainer, TrainingArguments, EarlyStoppingCallback, TrainerCallback
)
from sklearn.metrics import accuracy_score, f1_score
import os
import shutil
=======
    Trainer, TrainingArguments, EarlyStoppingCallback
)
from sklearn.metrics import accuracy_score, classification_report
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)

# è®¾ç½®è®¾å¤‡
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
print(f'ä½¿ç”¨è®¾å¤‡: {device}')

# åŠ è½½æ¨¡å‹å’Œåˆ†è¯å™¨
tokenizer = BertTokenizer.from_pretrained('./chinese_bert')
model = BertForSequenceClassification.from_pretrained('./chinese_bert', num_labels=3)
model.to(device)

# åŠ è½½è®­ç»ƒæ•°æ®å’ŒéªŒè¯æ•°æ®
train_df = pd.read_csv('train_data.csv')
val_df = pd.read_csv('val_data.csv')
print(f"è®­ç»ƒé›†: {len(train_df)}æ¡, éªŒè¯é›†: {len(val_df)}æ¡")

<<<<<<< HEAD
# æ˜¾ç¤ºæ•°æ®åˆ†å¸ƒ
print("è®­ç»ƒé›†åˆ†å¸ƒ:", train_df['rating'].value_counts().sort_index().to_dict())
print("éªŒè¯é›†åˆ†å¸ƒ:", val_df['rating'].value_counts().sort_index().to_dict())

# åŠ è½½å¹¶åŠ å¼ºç±»åˆ«æƒé‡
class_weights = np.load('class_weights.npy')
print("åŸå§‹ç±»åˆ«æƒé‡:", class_weights)

# åŠ å¼ºç±»åˆ«æƒé‡
class_weights = class_weights * 3
print("åŠ å¼ºåç±»åˆ«æƒé‡:", class_weights)
=======
# åŠ è½½ç±»åˆ«æƒé‡
class_weights = np.load('class_weights.npy')
print("ç±»åˆ«æƒé‡:", class_weights)
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)

# æ–‡æœ¬ç¼–ç 
def encode_texts(texts, labels):
    encodings = tokenizer(
        texts, 
        truncation=True, 
        padding=True, 
        max_length=128, 
        return_tensors='pt'
    )
    return {
        'input_ids': encodings['input_ids'],
        'attention_mask': encodings['attention_mask'], 
        'labels': torch.tensor(labels, dtype=torch.long)
    }

train_encodings = encode_texts(train_df['review_clean'].tolist(), train_df['rating'].tolist())
val_encodings = encode_texts(val_df['review_clean'].tolist(), val_df['rating'].tolist())

# åˆ›å»ºæ•°æ®é›†
class ReviewDataset(torch.utils.data.Dataset):
    def __init__(self, encodings):
        self.encodings = encodings
    def __len__(self):
        return len(self.encodings['labels'])
    def __getitem__(self, idx):
        return {k: v[idx] for k, v in self.encodings.items()}

train_dataset = ReviewDataset(train_encodings)
val_dataset = ReviewDataset(val_encodings)

<<<<<<< HEAD
# è‡ªå®šä¹‰Trainerä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡
class WeightedTrainer(Trainer):
    def __init__(self, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.best_f1_macro = 0
        self.best_f1_weighted = 0
        self.best_macro_checkpoint = None
        self.best_weighted_checkpoint = None
        
=======
# è‡ªå®šä¹‰Trainerä»¥å¤„ç†ç±»åˆ«ä¸å¹³è¡¡ - ä¿®å¤compute_lossæ–¹æ³•
class WeightedTrainer(Trainer):
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
    def compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):
        labels = inputs.get("labels")
        outputs = model(**inputs)
        logits = outputs.get("logits")
        
<<<<<<< HEAD
        # åº”ç”¨åŠ å¼ºåçš„ç±»åˆ«æƒé‡
=======
        # åº”ç”¨ç±»åˆ«æƒé‡
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
        loss_fct = torch.nn.CrossEntropyLoss(
            weight=torch.tensor(class_weights, dtype=torch.float).to(device)
        )
        loss = loss_fct(logits.view(-1, self.model.config.num_labels), labels.view(-1))
        
        return (loss, outputs) if return_outputs else loss

<<<<<<< HEAD
# è®¡ç®—è¯„ä¼°æŒ‡æ ‡ - åŒæ—¶ä½¿ç”¨å®å¹³å‡å’ŒåŠ æƒå¹³å‡F1åˆ†æ•°
=======
# è®¡ç®—è¯„ä¼°æŒ‡æ ‡
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
def compute_metrics(p):
    predictions, labels = p
    preds = np.argmax(predictions, axis=1)
    accuracy = accuracy_score(labels, preds)
<<<<<<< HEAD
    f1_macro = f1_score(labels, preds, average='macro')  
    f1_weighted = f1_score(labels, preds, average='weighted')
    
    return {
        'accuracy': accuracy,
        'f1_macro': f1_macro,
        'f1_weighted': f1_weighted
    }

# åˆ›å»ºä¿å­˜æœ€ä½³æ¨¡å‹çš„å›è°ƒ
class SaveBestModelsCallback(TrainerCallback):
    def __init__(self, tokenizer):
        super().__init__()
        self.tokenizer = tokenizer
        self.best_f1_macro = 0
        self.best_f1_weighted = 0
        
    def on_evaluate(self, args, state, control, metrics=None, **kwargs):
        # ä¿å­˜åŸºäºå®å¹³å‡F1çš„æœ€ä½³æ¨¡å‹
        if metrics and 'eval_f1_macro' in metrics:
            current_f1_macro = metrics['eval_f1_macro']
            if current_f1_macro > self.best_f1_macro:
                self.best_f1_macro = current_f1_macro
                print(f"æ–°çš„å®å¹³å‡F1æœ€ä½³æ¨¡å‹: {current_f1_macro:.4f}")
                # ä¿å­˜å®å¹³å‡æœ€ä½³æ¨¡å‹
                output_dir = os.path.join(args.output_dir, "best_f1_macro")
                kwargs['model'].save_pretrained(output_dir)
                self.tokenizer.save_pretrained(output_dir)
        
        # ä¿å­˜åŸºäºåŠ æƒå¹³å‡F1çš„æœ€ä½³æ¨¡å‹
        if metrics and 'eval_f1_weighted' in metrics:
            current_f1_weighted = metrics['eval_f1_weighted']
            if current_f1_weighted > self.best_f1_weighted:
                self.best_f1_weighted = current_f1_weighted
                print(f"æ–°çš„åŠ æƒå¹³å‡F1æœ€ä½³æ¨¡å‹: {current_f1_weighted:.4f}")
                # ä¿å­˜åŠ æƒå¹³å‡æœ€ä½³æ¨¡å‹
                output_dir = os.path.join(args.output_dir, "best_f1_weighted")
                kwargs['model'].save_pretrained(output_dir)
                self.tokenizer.save_pretrained(output_dir)

# è®­ç»ƒå‚æ•° - ä¸ä½¿ç”¨è‡ªåŠ¨ä¿å­˜æœ€ä½³æ¨¡å‹
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=96,  
    per_device_eval_batch_size=128,
    lr_scheduler_type='cosine',
    warmup_ratio=0.1, 
    weight_decay=0.01, 
    fp16=True,
    eval_strategy='epoch',
    save_strategy='epoch',
    save_total_limit=2,
    load_best_model_at_end=False,  # å…³é”®ï¼šå…³é—­è‡ªåŠ¨åŠ è½½æœ€ä½³æ¨¡å‹
    metric_for_best_model='f1_macro',
    greater_is_better=True,
    max_grad_norm=1.0,
    optim='adamw_torch_fused',
    dataloader_num_workers=6,  
)

# åˆ›å»ºå›è°ƒå®ä¾‹
save_best_callback = SaveBestModelsCallback(tokenizer)

# å¼€å§‹è®­ç»ƒ
=======
    return {'accuracy': accuracy}

# è®­ç»ƒå‚æ•°
training_args = TrainingArguments(
    output_dir='./results',
    num_train_epochs=10,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    eval_strategy='epoch',
    save_strategy='epoch',
    load_best_model_at_end=True,
    metric_for_best_model='accuracy',
    greater_is_better=True,
    weight_decay=0.01,
    warmup_steps=100,
    logging_steps=50,
)

# å¼€å§‹è®­ç»ƒï¼ˆå¢åŠ æ—©åœå›è°ƒï¼‰
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
trainer = WeightedTrainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
<<<<<<< HEAD
    callbacks=[EarlyStoppingCallback(early_stopping_patience=4), save_best_callback]
=======
    callbacks=[EarlyStoppingCallback(early_stopping_patience=3)]
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
)

print("å¼€å§‹è®­ç»ƒæ¨¡å‹...")
trainer.train()

<<<<<<< HEAD
print("è®­ç»ƒå®Œæˆï¼")
print(f"è®­ç»ƒè¿‡ç¨‹ä¸­ä¿å­˜çš„æœ€ä½³æ¨¡å‹:")
print(f"- å®å¹³å‡F1æœ€ä½³æ¨¡å‹: ./results/best_f1_macro (éªŒè¯é›†F1: {save_best_callback.best_f1_macro:.4f})")
print(f"- åŠ æƒå¹³å‡F1æœ€ä½³æ¨¡å‹: ./results/best_f1_weighted (éªŒè¯é›†F1: {save_best_callback.best_f1_weighted:.4f})")

# æ£€æŸ¥æ¨¡å‹æ˜¯å¦çœŸçš„ä¿å­˜äº†
print("\næ£€æŸ¥ä¿å­˜çš„æ¨¡å‹:")
for model_dir in ['./results/best_f1_macro', './results/best_f1_weighted']:
    if os.path.exists(model_dir):
        files = os.listdir(model_dir)
        print(f"âœ… {model_dir}: {len(files)}ä¸ªæ–‡ä»¶")
        for file in files:
            print(f"   - {file}")
    else:
        print(f"âŒ {model_dir}: ç›®å½•ä¸å­˜åœ¨")
=======
# ä¿å­˜æ¨¡å‹
trainer.save_model('./trained_model')
tokenizer.save_pretrained('./trained_model')
print("è®­ç»ƒå®Œæˆï¼")
>>>>>>> 155009d (ğŸ‰ init:é¡¹ç›®åˆç‰ˆ)
